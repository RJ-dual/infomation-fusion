from typing import List, Dict, Any, Optional, Tuple
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
import re
import json
import logging
from collections import Counter, defaultdict
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import argparse
import os
import sys
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
from sentence_transformers import SentenceTransformer
import torch

# å¯¼å…¥é…ç½®
from config import (
    get_model_deployment_config,
    DEFAULT_TEACHER_MODEL,
    MAX_WORKERS,
    REQUEST_TIMEOUT,
    LOG_LEVEL,
    DEBUG_MODE,
    ENABLE_PROGRESS_BAR,
    API_RETRY_LIMIT,
    COT_CONFIDENCE_THRESHOLD,
    LOW_CONFIDENCE_THRESHOLD,
    BATCH_SIZE,
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)

# ä¿®å¤jiebaæƒé™é—®é¢˜
jieba_cache_dir = os.path.join(os.path.expanduser("~"), ".jieba_cache")
os.makedirs(jieba_cache_dir, exist_ok=True)
jieba.dt.cache_file = os.path.join(jieba_cache_dir, "jieba.cache")
if DEBUG_MODE:
    logger.info(f"ğŸ”§ è®¾ç½®jiebaç¼“å­˜ç›®å½•: {jieba_cache_dir}")

# ==================== é—®é¢˜åˆ†ç±»å®šä¹‰ ====================

# ç¬¬ä¸€ç±»ï¼šæ•´ä½“æ–‡æœ¬é—®é¢˜ï¼ˆGlobal Problemsï¼‰
GLOBAL_PROBLEMS = {
    "é€»è¾‘è¿è´¯æ€§æ–­è£‚": {
        "description": "æè¿°ç¾éš¾äº‹ä»¶å‘å±•è¿‡ç¨‹æ—¶ï¼Œå¥å­æˆ–æ®µè½é—´ç¼ºä¹å¿…è¦é€»è¾‘è¿æ¥ï¼Œå¯¼è‡´ä¿¡æ¯æ”¯ç¦»ç ´ç¢ï¼Œéš¾ä»¥ç†è§£äº‹ä»¶å…¨è²Œã€‚",
        "evaluation_method": "é€šè¯»å®Œæ•´èåˆæ–‡æœ¬ï¼Œæ£€æŸ¥æ®µè½é—´é€»è¾‘è¿æ¥"
    },
    "è¯­è¨€é£æ ¼ä¸€è‡´æ€§é”™è¯¯": {
        "description": "èåˆç¾éš¾æŠ¥é“æ—¶ï¼Œæ–‡ä½“ã€è¯­æ€æˆ–ç”¨è¯­é£æ ¼å‘ç”Ÿçªç„¶å˜åŒ–ï¼Œåœ¨æ­£å¼å®˜æ–¹é€šæŠ¥ä¸å£è¯­åŒ–è¡¨è¾¾ä¹‹é—´è·³è·ƒï¼Œç ´åæ–‡æœ¬æ•´ä½“æ€§ã€‚",
        "evaluation_method": "æ£€æŸ¥å…¨æ–‡è¯­è¨€é£æ ¼æ˜¯å¦ç»Ÿä¸€"
    },
    "æ–‡æœ¬ç»“æ„ç»„ç»‡æ··ä¹±": {
        "description": "èåˆç¾éš¾ä¿¡æ¯æ—¶ï¼Œä¿¡æ¯ç»„ç»‡æ–¹å¼æ‚ä¹±æ— ç« ï¼ŒåŒç±»ä¿¡æ¯è¢«åˆ†æ•£åœ¨ä¸åŒéƒ¨åˆ†ï¼Œç¼ºä¹æ¸…æ™°é€»è¾‘ç»“æ„å’Œå±‚æ¬¡åˆ’åˆ†ã€‚",
        "evaluation_method": "è¯„ä¼°æ•´ä½“ä¿¡æ¯ç»„ç»‡ç»“æ„å’Œå±‚æ¬¡"
    },
    "æ—¶é—´é¡ºåºå‡†ç¡®æ€§é”™è¯¯": {
        "description": "èåˆç¾éš¾äº‹ä»¶æ—¶é—´çº¿ä¿¡æ¯æ—¶ï¼Œé”™è¯¯æ’åˆ—äº‹ä»¶å‘ç”Ÿçš„å…ˆåé¡ºåºã€‚",
        "evaluation_method": "å‚ç…§åŸå§‹æ–‡æœ¬è¿›è¡Œæ£€æŸ¥ï¼Œæ£€æŸ¥èåˆåçš„æ–‡æœ¬çš„æ—¶é—´çº¿é€»è¾‘æ˜¯å¦å’ŒåŸå§‹æ–‡æœ¬ä¸€è‡´ã€‚"
    },
    "å› æœå…³ç³»é€»è¾‘é”™è¯¯": {
        "description": "èåˆç¾éš¾ä¿¡æ¯æ—¶ï¼Œæ“…è‡ªæ·»åŠ æˆ–é¢ å€’äº‹ä»¶é—´çš„å› æœå…³ç³»ã€‚",
        "evaluation_method": "æ£€æŸ¥å…¨æ–‡å› æœå…³ç³»æ˜¯å¦åˆç†"
    }
}

# ç¬¬äºŒç±»ï¼šå¥å­çº§åˆ«é—®é¢˜ï¼ˆSentence-Level Problemsï¼‰
SENTENCE_LEVEL_PROBLEMS = {
    "äº‹å®å®Œæ•´æ€§ç¼ºå¤±": {
        "description": "åœ¨èåˆç¾éš¾äº‹ä»¶ä¿¡æ¯æ—¶ï¼Œæœªèƒ½åŒ…å«åŸæ–‡æœ¬ä¸­å­˜åœ¨çš„å…³é”®äº‹å®ç»†èŠ‚ï¼Œå¦‚å—ç¾èŒƒå›´ã€ä¼¤äº¡äººæ•°ã€ç¾å®³ç­‰çº§æˆ–æ•‘æ´è¿›å±•ï¼Œå¯¼è‡´ä¿¡æ¯ä¸å®Œæ•´ï¼Œå½±å“å¯¹ç¾æƒ…çš„å…¨é¢ç†è§£ã€‚",
        "evaluation_method": "å¯¹æ¯”å¥å­evidenceä¸èåˆæ–‡æœ¬ï¼Œæ£€æŸ¥å…³é”®äº‹å®æ˜¯å¦ç¼ºå¤±"
    },
    "æ ¸å¿ƒæ•°æ®å‡†ç¡®æ€§é”™è¯¯": {
        "description": "åœ¨èåˆç¾éš¾å…³é”®æ•°æ®æ—¶å‡ºç°æ•°å€¼é”™è¯¯ï¼ŒåŒ…æ‹¬ä¼¤äº¡äººæ•°ã€ç»æµæŸå¤±é‡‘é¢ã€éœ‡çº§ã€é£é€Ÿç­‰æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¯¼è‡´å¯¹ç¾æƒ…ä¸¥é‡ç¨‹åº¦çš„è¯¯åˆ¤ã€‚",
        "evaluation_method": "æ£€æŸ¥æ•°å­—ã€æ—¶é—´ç­‰æ ¸å¿ƒæ•°æ®æ˜¯å¦å‡†ç¡®"
    },
    "ä¿¡æ¯æ¥æºå½’å±é”™è¯¯": {
        "description": "åœ¨èåˆä¿¡æ¯æ—¶ï¼Œé”™è¯¯åœ°å½’å±æˆ–æ··æ·†ä¿¡æ¯çš„åŸå§‹æ¥æºæœºæ„ã€ä¸ªäººæˆ–å‡ºå¤„ï¼ŒåŒ…æ‹¬é”™è¯¯æŒ‡å®šå‘è¨€ä¸»ä½“ã€é”™è¯¯å¼•ç”¨æƒå¨æœºæ„ã€æˆ–é”™è¯¯æ ‡æ³¨ä¿¡æ¯å‡ºå¤„ã€‚",
        "evaluation_method": "æ£€æŸ¥äººç‰©ã€æœºæ„ç­‰å½’å±æ˜¯å¦æ­£ç¡®"
    },
    "è™šæ„å†…å®¹ç”Ÿæˆ": {
        "description": "åœ¨èåˆç¾éš¾ä¿¡æ¯æ—¶ï¼Œç”Ÿæˆä¸åŸå§‹æ–‡æœ¬æ— å…³æˆ–çŸ›ç›¾çš„è™šæ„å†…å®¹ï¼ŒåŒ…æ‹¬ç¼–é€ æœªå‘ç”Ÿçš„ç¾éš¾ã€è™šæ„é‡å¤§ä¼¤äº¡æˆ–ä¸å­˜åœ¨çš„æ•‘æ´è¡ŒåŠ¨ã€‚",
        "evaluation_method": "æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ— æ¥æºæ”¯æŒçš„å†…å®¹"
    },
    "ä¿¡æ¯è¡¨è¿°ç²¾ç¡®æ€§ä¸è¶³": {
        "description": "èåˆä¿¡æ¯æ—¶ï¼Œä½¿ç”¨æ¨¡ç³Šã€ä¸ç¡®å®šçš„è¯è¯­æ›¿ä»£åŸæœ¬å…·ä½“ã€æ˜ç¡®çš„ä¿¡æ¯è¡¨è¿°ï¼Œå¯¼è‡´å…³é”®äº‹å®ç»†èŠ‚ä¸¢å¤±ã€‚",
        "evaluation_method": "æ£€æŸ¥å…·ä½“ä¿¡æ¯æ˜¯å¦è¢«æ¨¡ç³ŠåŒ–"
    },
    "åˆ†ç±»å±‚çº§ä¸å½“": {
        "description": "èåˆä¿¡æ¯æ—¶ï¼Œä¸æ°å½“åœ°æ”¹å˜ä¿¡æ¯åœ¨åˆ†ç±»ä½“ç³»ä¸­çš„å±‚çº§ä½ç½®ï¼Œä¸»è¦è¡¨ç°ä¸ºå°†ç»†ç²’åº¦çš„å…·ä½“ç±»åˆ«å½’çº³ä¸ºç²—ç²’åº¦çš„ä¸Šä½ç±»åˆ«ï¼Œæˆ–é”™è¯¯åœ°è¿›è¡Œç±»åˆ«æ˜ å°„ï¼Œå¯¼è‡´åˆ†ç±»ä¿¡æ¯å¤±çœŸã€‚",
        "evaluation_method": "æ£€æŸ¥åˆ†ç±»æ¦‚å¿µæ˜¯å¦è¢«é”™è¯¯æå‡æˆ–é™ä½"
    },
    "å†…å®¹å®¢è§‚æ€§åå·®": {
        "description": "èåˆç¾éš¾ä¿¡æ¯æ—¶ï¼Œæ·»åŠ ä¸å¿…è¦çš„ã€å¸¦æœ‰å¼ºçƒˆä¸ªäººæˆ–æœºæ„å€¾å‘æ€§çš„è¯„ä»·æ€§è¯­è¨€å’Œä¸»è§‚åˆ¤æ–­ï¼Œåç¦»åŸæ–‡æœ¬çš„ç«‹åœºï¼Œå½±å“ä¿¡æ¯å®¢è§‚æ€§ã€‚",
        "evaluation_method": "æ£€æŸ¥æ˜¯å¦æ·»åŠ ä¸»è§‚è¯„ä»·"
    },
    "å†²çªä¿¡æ¯å¤„ç†ä¸å½“": {
        "description": "åœ¨èåˆå¤šä¸ªæ¥æºä¿¡æ¯æ—¶ï¼Œæœªèƒ½è¯†åˆ«æˆ–å¦¥å–„å¤„ç†æ¥æºé—´çš„æ˜æ˜¾å†²çªä¿¡æ¯ï¼ŒåŒ…æ‹¬å…³é”®äº‹å®çŸ›ç›¾ã€æ•°æ®ä¸ä¸€è‡´ã€æ—¶é—´å†²çªç­‰ï¼Œå¯¼è‡´èåˆç»“æœåŒ…å«æœªè§£å†³çš„çŸ›ç›¾æˆ–é”™è¯¯é‡‡çº³äº†å†²çªä¿¡æ¯ã€‚",
        "evaluation_method": "æ£€æŸ¥conflict_resolvedå­—æ®µæ˜¯å¦å……åˆ†å¤„ç†å†²çª,å¦‚è®¤ä¸ºå­˜åœ¨å†²çªï¼Œå¿…é¡»è¾“å‡ºå†²çªè¯æ®"
    }
}

# ==================== è¯„ä¼°æç¤ºæ¨¡æ¿ ====================

GLOBAL_EVALUATE_PROMPT = """
ä½œä¸ºä¸“ä¸šæ–°é—»ç¼–è¾‘ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹èåˆæ–‡æœ¬æ˜¯å¦å­˜åœ¨{problem_type}é—®é¢˜ã€‚

## é—®é¢˜æè¿°: {problem_description}

## è¯„ä¼°è¦æ±‚:
1. ä»”ç»†é˜…è¯»å®Œæ•´èåˆæ–‡æœ¬ï¼Œè¯„ä¼°æ•´ä½“è´¨é‡
2. åˆ¤æ–­æ˜¯å¦å­˜åœ¨{problem_type}é—®é¢˜
3. å¦‚æœå­˜åœ¨ï¼Œè¯·æä¾›å…·ä½“è¯æ®
4. å¦‚æœä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ

## æ–‡æœ¬å†…å®¹:
**åŸå§‹æ–‡æœ¬**: {original_text}
**èåˆæ–‡æœ¬**: {fused_text}

## è¾“å‡ºæ ¼å¼: è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
- "problem_exists": å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦å­˜åœ¨è¯¥é—®é¢˜
- "evidence": å…·ä½“è¯æ®ï¼ˆå¦‚æœå­˜åœ¨é—®é¢˜ï¼‰

è¯·å¼€å§‹è¯„ä¼°ï¼š
"""

SENTENCE_EVALUATE_PROMPT = """
ä½œä¸ºä¸“ä¸šæ–°é—»ç¼–è¾‘ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹èåˆå¥å­æ˜¯å¦å­˜åœ¨{problem_type}é—®é¢˜ã€‚

## é—®é¢˜æè¿°: {problem_description}

## è¯„ä¼°è¦æ±‚:
1. ä»”ç»†å¯¹æ¯”èåˆå¥å­ä¸åŸå§‹è¯æ®
2. åˆ¤æ–­æ˜¯å¦å­˜åœ¨{problem_type}é—®é¢˜
3. å¦‚æœå­˜åœ¨ï¼Œè¯·æä¾›å…·ä½“è¯æ®
4. å¦‚æœä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ

## å¥å­ä¿¡æ¯:
**èåˆå¥å­**: {fused_sentence}
**åŸå§‹è¯æ®**: {evidence_texts}

## å¯¹é½ä¿¡æ¯:
- æ¥æºç‰ˆæœ¬: {sources}
- å†²çªå¤„ç†: {conflict_resolved}

## è¾“å‡ºæ ¼å¼: è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
- "problem_exists": å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦å­˜åœ¨è¯¥é—®é¢˜
- "evidence": å…·ä½“è¯æ®ï¼ˆå¦‚æœå­˜åœ¨é—®é¢˜ï¼‰

è¯·å¼€å§‹è¯„ä¼°ï¼š
"""

class TextSimilarityCalculator:
    """æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å™¨"""

    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2", local_model_path: str = None, gpu_device: str = "cuda:0"):
        """ åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨
        Args:
            model_name: ä½¿ç”¨çš„å¥å­è½¬æ¢æ¨¡å‹åç§°
            local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
            gpu_device: æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ï¼Œå¦‚ "cuda:0", "cuda:1" ç­‰
        """
        self.model_name = model_name
        self.local_model_path = local_model_path
        self.gpu_device = gpu_device
        self.sentence_model = None
        self.tfidf_vectorizer = None
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()

    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            import torch
            # è®¾ç½®è®¾å¤‡
            if self.gpu_device and torch.cuda.is_available():
                # æ£€æŸ¥æŒ‡å®šçš„GPUæ˜¯å¦å¯ç”¨
                gpu_id = int(self.gpu_device.split(":")[1])
                if gpu_id < torch.cuda.device_count():
                    self.device = torch.device(self.gpu_device)
                    logger.info(f"âœ… ä½¿ç”¨è®¾å¤‡: {self.device}")
                else:
                    logger.warning(f"âš ï¸ æŒ‡å®šçš„GPUè®¾å¤‡ {self.gpu_device} ä¸å¯ç”¨ï¼Œå¯ç”¨çš„GPUæ•°é‡: {torch.cuda.device_count()}ï¼Œå°†ä½¿ç”¨CPU")
                    self.device = torch.device("cpu")
            else:
                self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"âœ… è‡ªåŠ¨é€‰æ‹©è®¾å¤‡: {self.device}")
            # åˆå§‹åŒ–å¥å­è½¬æ¢æ¨¡å‹ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
            if self.local_model_path and os.path.exists(self.local_model_path):
                self.sentence_model = SentenceTransformer(self.local_model_path, device=self.device)
                logger.info(f"âœ… ä»æœ¬åœ°è·¯å¾„åŠ è½½å¥å­è½¬æ¢æ¨¡å‹: {self.local_model_path}")
            else:
                self.sentence_model = SentenceTransformer(self.model_name, device=self.device)
                logger.info(f"âœ… å¥å­è½¬æ¢æ¨¡å‹ {self.model_name} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ å¥å­è½¬æ¢æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}, å°†ä½¿ç”¨TF-IDF")
            self.sentence_model = None
        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        self.tfidf_vectorizer = TfidfVectorizer(
            tokenizer=self._tokenize_chinese, min_df=1, max_df=0.8, ngram_range=(1, 2)
        )

    def _tokenize_chinese(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        return list(jieba.cut(text))

    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å¥å­è½¬æ¢æ¨¡å‹ï¼‰ """
        if not text1.strip() or not text2.strip():
            return 0.0
        try:
            if self.sentence_model is not None:
                embeddings = self.sentence_model.encode([text1, text2])
                similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
                return float(similarity)
            else:
                return self.calculate_tfidf_similarity(text1, text2)
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return self.calculate_tfidf_similarity(text1, text2)

    def calculate_tfidf_similarity(self, text1: str, text2: str) -> float:
        """ è®¡ç®—TF-IDFç›¸ä¼¼åº¦ """
        if not text1.strip() or not text2.strip():
            return 0.0
        try:
            corpus = [text1, text2]
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(similarity)
        except Exception as e:
            logger.error(f"âŒ TF-IDFç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:
        """ è®¡ç®—Jaccardç›¸ä¼¼åº¦ """
        if not text1.strip() or not text2.strip():
            return 0.0
        try:
            words1 = set(self._tokenize_chinese(text1))
            words2 = set(self._tokenize_chinese(text2))
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            return intersection / union if union > 0 else 0.0
        except Exception as e:
            logger.error(f"âŒ Jaccardç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def calculate_levenshtein_similarity(self, text1: str, text2: str) -> float:
        """ è®¡ç®—åŸºäºç¼–è¾‘è·ç¦»çš„ç›¸ä¼¼åº¦ """
        if not text1.strip() or not text2.strip():
            return 0.0
        try:
            if text1 == text2:
                return 1.0
            len1, len2 = len(text1), len(text2)
            max_len = max(len1, len2)
            if max_len == 0:
                return 1.0
            distance = self._levenshtein_distance(text1, text2)
            return 1.0 - (distance / max_len)
        except Exception as e:
            logger.error(f"âŒ ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—Levenshteinç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        return previous_row[-1]

    def calculate_comprehensive_similarity(self, text1: str, text2: str) -> Dict[str, float]:
        """ è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦ï¼ˆå¤šç§æ–¹æ³•ï¼‰ """
        return {
            "semantic_similarity": self.calculate_semantic_similarity(text1, text2),
            "tfidf_similarity": self.calculate_tfidf_similarity(text1, text2),
            "jaccard_similarity": self.calculate_jaccard_similarity(text1, text2),
            "levenshtein_similarity": self.calculate_levenshtein_similarity(text1, text2),
        }

    def calculate_fused_text_similarities(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """ è®¡ç®—èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬åŠæ”¹å†™ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦ """
        try:
            fused_content = news_item.get("fused_content", "")
            original_content = news_item.get("input_text", "")
            if not fused_content.strip() or not original_content.strip():
                logger.warning("âŒ èåˆæ–‡æœ¬æˆ–åŸå§‹æ–‡æœ¬ä¸ºç©º")
                return self._create_empty_similarity_result()
            similarities = {
                "original": self.calculate_comprehensive_similarity(fused_content, original_content)
            }
            rewrite_versions = {}
            for i in range(1, 4):
                rewrite_key_v = f"rewritten_v{i}"
                rewrite_key_n = f"rewrite_{i}"
                if rewrite_key_v in news_item and news_item[rewrite_key_v]:
                    rewrite_versions[rewrite_key_v] = news_item[rewrite_key_v]
                elif rewrite_key_n in news_item and news_item[rewrite_key_n]:
                    rewrite_versions[rewrite_key_n] = news_item[rewrite_key_n]
            for version_key, rewrite_content in rewrite_versions.items():
                if rewrite_content and rewrite_content.strip():
                    try:
                        similarities[version_key] = self.calculate_comprehensive_similarity(fused_content, rewrite_content)
                    except Exception as e:
                        logger.warning(f"âš ï¸ è®¡ç®—ä¸{version_key}çš„ç›¸ä¼¼åº¦å¤±è´¥: {e}")
                        similarities[version_key] = {"comprehensive_score": 0.0, "calculation_success": False}
            avg_similarities = self._calculate_average_similarities(similarities)
            analysis = self._generate_similarity_analysis(similarities, avg_similarities)
            return {
                "similarities": similarities,
                "average_similarities": avg_similarities,
                "analysis": analysis,
                "similarity_calculation_success": True,
            }
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return self._create_empty_similarity_result()

    def _calculate_average_similarities(self, similarities: Dict[str, Dict[str, float]]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦"""
        avg_similarities = {}
        methods = ["semantic_similarity", "tfidf_similarity", "jaccard_similarity", "levenshtein_similarity"]
        for method in methods:
            scores = []
            for version_similarities in similarities.values():
                if method in version_similarities:
                    scores.append(version_similarities[method])
            if scores:
                avg_similarities[method] = sum(scores) / len(scores)
            else:
                avg_similarities[method] = 0.0
        return avg_similarities

    def _generate_similarity_analysis(self, similarities: Dict[str, Dict[str, float]], avg_similarities: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š"""
        analysis = {"overall_assessment": "", "key_observations": [], "recommendations": []}
        semantic_avg = avg_similarities.get("semantic_similarity", 0)
        if semantic_avg >= 0.8:
            analysis["overall_assessment"] = "èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬åŠæ”¹å†™ç‰ˆæœ¬é«˜åº¦ç›¸ä¼¼ï¼Œä¿¡æ¯ä¿ç•™å®Œæ•´"
        elif semantic_avg >= 0.6:
            analysis["overall_assessment"] = "èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬åŠæ”¹å†™ç‰ˆæœ¬ä¸­ç­‰ç›¸ä¼¼ï¼Œä¸»è¦ä¿¡æ¯å¾—åˆ°ä¿ç•™"
        elif semantic_avg >= 0.4:
            analysis["overall_assessment"] = "èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬åŠæ”¹å†™ç‰ˆæœ¬ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå­˜åœ¨ä¿¡æ¯ä¸¢å¤±"
        else:
            analysis["overall_assessment"] = "èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬åŠæ”¹å†™ç‰ˆæœ¬ç›¸ä¼¼åº¦å¾ˆä½ï¼Œä¿¡æ¯ä¿ç•™ä¸å®Œæ•´"
        if "original" in similarities:
            orig_semantic = similarities["original"].get("semantic_similarity", 0)
            if orig_semantic < 0.5:
                analysis["key_observations"].append("èåˆæ–‡æœ¬ä¸åŸå§‹æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨é‡è¦ä¿¡æ¯é—æ¼")
        method_scores = list(avg_similarities.values())
        if len(method_scores) >= 2:
            score_variance = np.var(method_scores)
            if score_variance > 0.1:
                analysis["key_observations"].append("ä¸åŒç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ç»“æœå·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®å…³æ³¨è¯­ä¹‰ç›¸ä¼¼åº¦")
        if semantic_avg < 0.6:
            analysis["recommendations"].append("å»ºè®®æ£€æŸ¥èåˆæ–‡æœ¬æ˜¯å¦ä¿ç•™äº†åŸå§‹æ–‡æœ¬çš„å…³é”®ä¿¡æ¯")
        if avg_similarities.get("jaccard_similarity", 0) < 0.3:
            analysis["recommendations"].append("è¯æ±‡é‡å åº¦è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¿‡å¤šä¸åŒçš„è¡¨è¾¾æ–¹å¼")
        return analysis

    def _create_empty_similarity_result(self) -> Dict[str, Any]:
        """åˆ›å»ºç©ºçš„ç›¸ä¼¼åº¦ç»“æœ"""
        return {
            "similarities": {},
            "average_similarities": {},
            "analysis": {
                "overall_assessment": "ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥",
                "key_observations": ["æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦"],
                "recommendations": ["è¯·æ£€æŸ¥è¾“å…¥æ–‡æœ¬"],
            },
            "similarity_calculation_success": False,
        }

class DualEvaluationSystem:
    """åŒé‡è¯„ä¼°ç³»ç»Ÿ - æ”¯æŒæ•´ä½“æ–‡æœ¬è¯„ä¼°å’Œå¥å­çº§åˆ«è¯„ä¼°"""

    def __init__(self, strict_model_config: Dict[str, Any], cot_model_config: Dict[str, Any] = None, local_similarity_model_path: str = None, gpu_device: str = "cuda:0"):
        self.strict_model_config = strict_model_config
        self.cot_model_config = cot_model_config if cot_model_config else strict_model_config.copy()
        self.strict_model_name = strict_model_config.get("model", "unknown")
        self.cot_model_name = self.cot_model_config.get("model", "unknown")
        self.retry_limit = API_RETRY_LIMIT
        self.gpu_device = gpu_device
        
        if DEBUG_MODE:
            logger.info(f"ğŸ¤– åˆå§‹åŒ–åŒé‡è¯„ä¼°ç³»ç»Ÿï¼Œä¸¥æ ¼æ¨¡å‹: {self.strict_model_name}, CoTæ¨¡å‹: {self.cot_model_name}")
            logger.info(f"ğŸ¯ æŒ‡å®šGPUè®¾å¤‡: {gpu_device}")
            
        self.llm_strict = self._initialize_llm(strict_model_config)
        self.llm_cot = self._initialize_llm(self.cot_model_config)
        self.similarity_calculator = TextSimilarityCalculator(local_model_path=local_similarity_model_path, gpu_device=gpu_device)

    def _initialize_llm(self, model_config: Dict[str, Any]) -> ChatOpenAI:
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹"""
        temperature = model_config.get("temperature", 0.7)
        max_tokens = model_config.get("max_tokens", 4000)
        model_name = model_config.get("model_name", model_config.get("model", "gpt-4"))
        api_key = model_config.get("api_key", "")
        base_url = model_config.get("base_url", "https://api.openai.com/v1")
        
        if api_key == "EMPTY" or api_key == "vllm":
            api_key = "vllm"
            if not base_url or base_url == "https://api.openai.com/v1":
                base_url = os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1")
            logger.info(f"ğŸ”§ ä½¿ç”¨vllmæœ¬åœ°æ¨¡å‹: {model_name}, åœ°å€: {base_url}")
            
        if DEBUG_MODE:
            logger.info(f"ğŸ”§ åˆå§‹åŒ–LLM: æ¨¡å‹={model_name}, æ¸©åº¦={temperature}, æœ€å¤§ä»¤ç‰Œ={max_tokens}, åŸºç¡€URL={base_url}")
            
        return ChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            openai_api_key=api_key,
            openai_api_base=base_url,
            request_timeout=REQUEST_TIMEOUT,
            max_retries=self.retry_limit,
        )

    def safe_llm_call(self, messages, llm_instance=None, max_retries=3, base_delay=1):
        """å®‰å…¨çš„LLMè°ƒç”¨"""
        llm = llm_instance if llm_instance else self.llm_strict
        for attempt in range(max_retries + 1):
            try:
                response = llm.invoke(messages)
                return response
            except Exception as e:
                if attempt < max_retries:
                    delay = base_delay * (2 ** attempt)
                    if DEBUG_MODE:
                        logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œç¬¬{attempt + 1}æ¬¡é‡è¯•")
                    time.sleep(delay)
                else:
                    model_name = self.strict_model_name if llm_instance is None else self.cot_model_name
                    logger.error(f"âŒ {model_name} APIè°ƒç”¨å¤±è´¥")
        return None

    def safe_json_parse(self, response_text: str) -> Dict[str, Any]:
        """å®‰å…¨è§£æJSONå“åº”"""
        if isinstance(response_text, dict):
            return response_text
        if isinstance(response_text, str):
            try:
                return json.loads(response_text)
            except:
                json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
                if json_match:
                    try:
                        return json.loads(json_match.group())
                    except:
                        pass
        return {"problem_exists": False, "evidence": ""}

    # ==================== æ•´ä½“æ–‡æœ¬è¯„ä¼°æ–¹æ³• ====================

    def evaluate_global_problem(self, problem_type: str, original_text: str, fused_text: str) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä½“æ–‡æœ¬é—®é¢˜"""
        try:
            problem_info = GLOBAL_PROBLEMS.get(problem_type, {})
            description = problem_info.get("description", "")
            
            prompt = GLOBAL_EVALUATE_PROMPT.format(
                problem_type=problem_type,
                problem_description=description,
                original_text=original_text,
                fused_text=fused_text,
            )
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ–°é—»ç¼–è¾‘ï¼Œè´Ÿè´£è¯„ä¼°æ–‡æœ¬èåˆçš„æ•´ä½“è´¨é‡ã€‚"),
                HumanMessage(content=prompt),
            ]
            
            response = self.safe_llm_call(messages, llm_instance=self.llm_strict)
            if not response:
                return {"problem_exists": False, "problem_type": problem_type, "evidence": "è¯„ä¼°å¤±è´¥"}
                
            eval_data = self.safe_json_parse(response.content)
            problem_exists = eval_data.get("problem_exists", False)
            evidence = eval_data.get("evidence", "")
            
            return {
                "problem_exists": problem_exists, 
                "problem_type": problem_type, 
                "evidence": evidence,
                "evaluation_method": "global_assessment"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ•´ä½“è¯„ä¼°å¤±è´¥ {problem_type}: {e}")
            return {"problem_exists": False, "problem_type": problem_type, "evidence": f"å¼‚å¸¸: {e}"}

    def global_evaluation_stage(self, original_text: str, fused_text: str) -> Dict[str, Any]:
        """é˜¶æ®µ1: æ•´ä½“æ–‡æœ¬è¯„ä¼°"""
        logger.info("ğŸŒ é˜¶æ®µ1: æ•´ä½“æ–‡æœ¬è¯„ä¼°")
        if not original_text.strip() or not fused_text.strip():
            return {"global_errors": [], "total_global_problems": 0}
            
        global_errors = []
        
        # å¹¶è¡Œå¯¹æ¯ä¸ªæ•´ä½“é—®é¢˜ç±»å‹è¿›è¡Œè¯„ä¼°
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(GLOBAL_PROBLEMS))) as executor:
            future_to_problem = {
                executor.submit(self.evaluate_global_problem, problem_type, original_text, fused_text): problem_type
                for problem_type in GLOBAL_PROBLEMS.keys()
            }
            
            for future in as_completed(future_to_problem):
                problem_type = future_to_problem[future]
                try:
                    result = future.result()
                    if result.get("problem_exists", False):
                        global_errors.append({
                            "error_type": result["problem_type"],
                            "evidence": result.get("evidence", ""),
                            "original_text": original_text,
                            "fused_text": fused_text,
                        })
                except Exception as e:
                    logger.error(f"âŒ æ•´ä½“é—®é¢˜ç±»å‹è¯„ä¼°å¼‚å¸¸: {problem_type}, {e}")
                    
        return {"global_errors": global_errors, "total_global_problems": len(global_errors)}

    # ==================== å¥å­çº§åˆ«è¯„ä¼°æ–¹æ³• ====================

    def evaluate_sentence_problem(self, problem_type: str, sentence_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å¥å­çº§åˆ«é—®é¢˜"""
        try:
            problem_info = SENTENCE_LEVEL_PROBLEMS.get(problem_type, {})
            description = problem_info.get("description", "")
            
            fused_sentence = sentence_data.get("text", "")
            evidence_texts = sentence_data.get("evidence", [])
            sources = sentence_data.get("sources", [])
            conflict_resolved = sentence_data.get("conflict_resolved", "æœªå¤„ç†")
            

            prompt = SENTENCE_EVALUATE_PROMPT.format(
                problem_type=problem_type,
                problem_description=description,
                fused_sentence=fused_sentence,
                evidence_texts=json.dumps(evidence_texts, ensure_ascii=False, indent=2),
                sources=sources,
                conflict_resolved=conflict_resolved,
            )
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ–°é—»ç¼–è¾‘ï¼Œè´Ÿè´£è¯„ä¼°å¥å­çº§åˆ«çš„èåˆè´¨é‡ã€‚"),
                HumanMessage(content=prompt),
            ]
            
            response = self.safe_llm_call(messages, llm_instance=self.llm_strict)
            if not response:
                return {"problem_exists": False, "problem_type": problem_type, "evidence": "è¯„ä¼°å¤±è´¥"}
                
            eval_data = self.safe_json_parse(response.content)
            problem_exists = eval_data.get("problem_exists", False)
            evidence = eval_data.get("evidence", "")
            
            return {
                "problem_exists": problem_exists, 
                "problem_type": problem_type, 
                "evidence": evidence,
                "sentence_idx": sentence_data.get("sent_idx"),
                "evaluation_method": "sentence_assessment"
            }
            
        except Exception as e:
            logger.error(f"âŒ å¥å­è¯„ä¼°å¤±è´¥ {problem_type}: {e}")
            return {"problem_exists": False, "problem_type": problem_type, "evidence": f"å¼‚å¸¸: {e}"}

    
    def sentence_evaluation_stage(self, alignment_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é˜¶æ®µ2: å¥å­çº§åˆ«è¯„ä¼°"""
        logger.info("ğŸ“ é˜¶æ®µ2: å¥å­çº§åˆ«è¯„ä¼°")
        if not alignment_data:
            return {"sentence_errors": [], "total_sentence_problems": 0}
            
        sentence_errors = []
        evaluation_tasks = []
        
        # ä¸ºæ¯ä¸ªå¥å­çš„æ¯ä¸ªé—®é¢˜ç±»å‹åˆ›å»ºè¯„ä¼°ä»»åŠ¡
        for sentence in alignment_data:
            for problem_type in SENTENCE_LEVEL_PROBLEMS.keys():
                evaluation_tasks.append({
                    "problem_type": problem_type,
                    "sentence_data": sentence
                })
        
        # å¹¶è¡Œæ‰§è¡Œå¥å­çº§åˆ«è¯„ä¼°
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(evaluation_tasks))) as executor:
            future_to_task = {
                executor.submit(self.evaluate_sentence_problem, task["problem_type"], task["sentence_data"]): task
                for task in evaluation_tasks
            }
            
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    if result.get("problem_exists", False):
                        sentence_errors.append({
                            "error_type": result["problem_type"],
                            "evidence": result.get("evidence", ""),
                            "sentence_idx": result.get("sentence_idx"),
                            "sentence_text": task["sentence_data"].get("text", ""),
                            "evaluation_method": result.get("evaluation_method", "")
                        })
                except Exception as e:
                    logger.error(f"âŒ å¥å­çº§åˆ«è¯„ä¼°å¼‚å¸¸: {e}")
                    
        return {"sentence_errors": sentence_errors, "total_sentence_problems": len(sentence_errors)}

    # ==================== ç›¸ä¼¼åº¦åˆ†æ ====================

    def similarity_analysis_stage(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """é˜¶æ®µ3: ç›¸ä¼¼åº¦åˆ†æ"""
        logger.info("ğŸ” é˜¶æ®µ3: ç›¸ä¼¼åº¦åˆ†æ")
        try:
            similarity_result = self.similarity_calculator.calculate_fused_text_similarities(news_item)
            return similarity_result
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")
            return self.similarity_calculator._create_empty_similarity_result()

    # ==================== ä¸»è¯„ä¼°æµç¨‹ ====================

    def extract_alignment_data(self, fused_content: str) -> List[Dict[str, Any]]:
        """ä»èåˆå†…å®¹ä¸­æå–å¯¹é½æ•°æ®"""
        try:
            # æŸ¥æ‰¾ALIGNMENTåˆ†éš”ç¬¦
            alignment_separator = "====ALIGNMENT===="
            if alignment_separator in fused_content:
                parts = fused_content.split(alignment_separator)
                if len(parts) > 1:
                    alignment_text = parts[1].strip()
                    # è§£æJSONæ•°ç»„
                    alignment_data = json.loads(alignment_text)
                    return alignment_data
            return []
        except Exception as e:
            logger.error(f"âŒ æå–å¯¹é½æ•°æ®å¤±è´¥: {e}")
            return []

    def dual_evaluation(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """åŒé‡è¯„ä¼°ä¸»æµç¨‹"""
        logger.info("ğŸ”„ å¼€å§‹åŒé‡è¯„ä¼°ï¼ˆæ•´ä½“æ–‡æœ¬ + å¥å­çº§åˆ«ï¼‰")
        
        original_text = news_item.get("input_text", "")
        fused_text = news_item.get("fused_content", "")
        
        if not original_text.strip() or not fused_text.strip():
            logger.warning(f"âŒ æ–°é—»å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°")
            return {"evaluation_success": False, "error": "åŸå§‹æ–‡æœ¬æˆ–èåˆæ–‡æœ¬ä¸ºç©º"}
        
        # é˜¶æ®µ1: æ•´ä½“æ–‡æœ¬è¯„ä¼°
        global_results = self.global_evaluation_stage(original_text, fused_text)
        
        # é˜¶æ®µ2: å¥å­çº§åˆ«è¯„ä¼°
        alignment_data = self.extract_alignment_data(fused_text)
        sentence_results = self.sentence_evaluation_stage(alignment_data)
        
        # é˜¶æ®µ3: ç›¸ä¼¼åº¦åˆ†æ
        similarity_results = self.similarity_analysis_stage(news_item)
        
        # ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡
        global_classification = self._classify_global_problems(global_results["global_errors"])
        sentence_classification = self._classify_sentence_problems(sentence_results["sentence_errors"])
        
        result = {
            "evaluation_success": True,
            "global_evaluation": global_results,
            "sentence_evaluation": sentence_results,
            "similarity_analysis": similarity_results,
            "problem_classification": {
                "global_problems": global_classification,
                "sentence_problems": sentence_classification,
                "total_problems": global_results["total_global_problems"] + sentence_results["total_sentence_problems"]
            }
        }
        
        logger.info(f"âœ… åŒé‡è¯„ä¼°å®Œæˆ:")
        logger.info(f" æ•´ä½“æ–‡æœ¬é—®é¢˜: {global_results['total_global_problems']}ä¸ª")
        logger.info(f" å¥å­çº§åˆ«é—®é¢˜: {sentence_results['total_sentence_problems']}ä¸ª")
        logger.info(f" æ€»é—®é¢˜æ•°: {result['problem_classification']['total_problems']}ä¸ª")
        
        if similarity_results.get("similarity_calculation_success", False):
            avg_similarities = similarity_results.get("average_similarities", {})
            semantic_sim = avg_similarities.get("semantic_similarity", 0)
            logger.info(f" å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.3f}")
            
        return result

    def _classify_global_problems(self, global_errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ç±»æ•´ä½“æ–‡æœ¬é—®é¢˜"""
        classification = defaultdict(int)
        for error in global_errors:
            error_type = error.get("error_type", "æœªçŸ¥ç±»å‹")
            classification[error_type] += 1
            
        return {
            "by_type": dict(classification),
            "total_count": len(global_errors)
        }

    def _classify_sentence_problems(self, sentence_errors: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ç±»å¥å­çº§åˆ«é—®é¢˜"""
        classification = defaultdict(int)
        for error in sentence_errors:
            error_type = error.get("error_type", "æœªçŸ¥ç±»å‹")
            classification[error_type] += 1
            
        return {
            "by_type": dict(classification),
            "total_count": len(sentence_errors)
        }

# ==================== è¯„ä¼°å¤„ç†å™¨ ====================

def create_dual_evaluator(strict_model_name: str = None, cot_model_name: str = None, local_similarity_model_path: str = None, gpu_device: str = "cuda:0"):
    """åˆ›å»ºåŒé‡è¯„ä¼°å™¨"""
    if strict_model_name is None:
        strict_model_name = DEFAULT_TEACHER_MODEL
    if cot_model_name is None:
        cot_model_name = strict_model_name
        
    strict_model_config = get_model_deployment_config(strict_model_name)
    cot_model_config = get_model_deployment_config(cot_model_name)
    
    return DualEvaluationSystem(strict_model_config, cot_model_config, local_similarity_model_path=local_similarity_model_path, gpu_device=gpu_device)

class DualEvaluationProcessor:
    """åŒé‡è¯„ä¼°å¤„ç†å™¨"""

    def __init__(self, strict_model_name: str = None, cot_model_name: str = None, local_similarity_model_path: str = None, gpu_device: str = "cuda:0"):
        self.evaluator = create_dual_evaluator(strict_model_name, cot_model_name, local_similarity_model_path=local_similarity_model_path, gpu_device=gpu_device)
        self.max_workers = max(1, MAX_WORKERS // 2)
        self.batch_size = max(1, BATCH_SIZE // 2)

    def process_single_news(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ç¯‡æ–°é—»çš„åŒé‡è¯„ä¼°"""
        try:
            evaluation_result = self.evaluator.dual_evaluation(news_item)
            return self._create_evaluated_news(news_item, evaluation_result)
        except Exception as e:
            logger.error(f"âŒ åŒé‡è¯„ä¼°å¤±è´¥: {e}")
            return self._create_evaluated_news(news_item, {"error": f"è¯„ä¼°å¼‚å¸¸: {str(e)}", "evaluation_success": False})

    def _create_evaluated_news(self, original_news: Dict[str, Any], evaluation_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºåŒ…å«è¯„ä¼°ç»“æœçš„æ–°é—»æ•°æ®"""
        evaluated_news = original_news.copy()
        evaluated_news.update({
            "dual_evaluation": evaluation_result,
            "evaluation_success": evaluation_result.get("evaluation_success", False),
        })
        
        if evaluation_result.get("evaluation_success", False):
            evaluated_news.update({
                "global_problem_count": evaluation_result["global_evaluation"]["total_global_problems"],
                "sentence_problem_count": evaluation_result["sentence_evaluation"]["total_sentence_problems"],
                "total_problem_count": evaluation_result["problem_classification"]["total_problems"],
            })
            
        similarity_analysis = evaluation_result.get("similarity_analysis", {})
        if similarity_analysis.get("similarity_calculation_success", False):
            avg_similarities = similarity_analysis.get("average_similarities", {})
            evaluated_news.update({
                "semantic_similarity": avg_similarities.get("semantic_similarity", 0),
                "tfidf_similarity": avg_similarities.get("tfidf_similarity", 0),
                "jaccard_similarity": avg_similarities.get("jaccard_similarity", 0),
            })
            
        return evaluated_news

    def process_news_batch(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†åŒé‡è¯„ä¼°"""
        logger.info(f"å¼€å§‹æ‰¹é‡åŒé‡è¯„ä¼° {len(news_list)} ç¯‡æ–°é—»")
        processed_news = []
        success_count = 0
        total_global_problems = 0
        total_sentence_problems = 0
        total_semantic_similarity = 0
        
        batch_size = min(self.batch_size, 2)
        
        for batch_start in range(0, len(news_list), batch_size):
            batch_end = min(batch_start + batch_size, len(news_list))
            batch = news_list[batch_start:batch_end]
            logger.info(f"ğŸ”„ åŒé‡è¯„ä¼°æ‰¹æ¬¡ {batch_start//batch_size + 1}: {len(batch)} ç¯‡æ–°é—»")
            
            batch_results = []
            pbar = tqdm(total=len(batch), desc=f"åŒé‡è¯„ä¼°æ‰¹æ¬¡ {batch_start//batch_size + 1}") if ENABLE_PROGRESS_BAR else None
            
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(batch))) as executor:
                future_to_news = {executor.submit(self.process_single_news, news): news for news in batch}
                
                for future in as_completed(future_to_news):
                    news_item = future_to_news[future]
                    try:
                        result = future.result()
                        batch_results.append(result)
                        
                        if result.get("evaluation_success", False):
                            success_count += 1
                            total_global_problems += result.get("global_problem_count", 0)
                            total_sentence_problems += result.get("sentence_problem_count", 0)
                            semantic_sim = result.get("semantic_similarity", 0)
                            total_semantic_similarity += semantic_sim
                            
                    except Exception as e:
                        logger.error(f"âŒ åŒé‡è¯„ä¼°æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                        error_news = self._create_evaluated_news(news_item, {"error": f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {str(e)}", "evaluation_success": False})
                        batch_results.append(error_news)
                    finally:
                        if pbar:
                            pbar.update(1)
                            
            if pbar:
                pbar.close()
                
            processed_news.extend(batch_results)
            batch_success = sum(1 for news in batch_results if news.get("evaluation_success", False))
            logger.info(f"âœ… åŒé‡è¯„ä¼°æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ {batch_success}/{len(batch_results)}")
            
        avg_semantic_similarity = total_semantic_similarity / success_count if success_count > 0 else 0
        logger.info(f"ğŸ‰ åŒé‡æ‰¹é‡è¯„ä¼°å®Œæˆ! æ€»è®¡: {len(processed_news)} ç¯‡, æˆåŠŸ: {success_count}")
        logger.info(f"ğŸ“Š æ€»æ•´ä½“é—®é¢˜: {total_global_problems}, æ€»å¥å­é—®é¢˜: {total_sentence_problems}")
        logger.info(f"ğŸ” å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {avg_semantic_similarity:.3f}")
        
        return processed_news

# ==================== å·¥å…·å‡½æ•° ====================

def load_news_for_evaluation(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½æ–°é—»æ•°æ®ç”¨äºè¯„ä¼°"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
        if isinstance(news_data, list):
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(news_data)} ç¯‡æ–°é—»")
            return news_data
        else:
            logger.error("âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯")
            return []
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        return []

def save_evaluated_news(news_list: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    try:
        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else '.', exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(news_list)} ç¯‡è¯„ä¼°æ–°é—»")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

def generate_dual_summary_report(news_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”ŸæˆåŒé‡è¯„ä¼°æ‘˜è¦æŠ¥å‘Š"""
    successful_evaluations = [news for news in news_list if news.get("evaluation_success", False)]
    if not successful_evaluations:
        return {
            "summary": {
                "total_news_evaluated": 0,
                "average_global_problems": 0.0,
                "average_sentence_problems": 0.0,
                "average_total_problems": 0.0,
                "average_semantic_similarity": 0.0
            },
            "global_problems_classification": {"by_type": {}, "total_count": 0},
            "sentence_problems_classification": {"by_type": {}, "total_count": 0},
        }
        
    total_news = len(successful_evaluations)
    global_problems_by_type = defaultdict(int)
    sentence_problems_by_type = defaultdict(int)
    semantic_similarities = []
    
    for news in successful_evaluations:
        evaluation = news.get("dual_evaluation", {})
        problem_classification = evaluation.get("problem_classification", {})
        
        # ç»Ÿè®¡æ•´ä½“é—®é¢˜
        global_problems = problem_classification.get("global_problems", {})
        for problem_type, count in global_problems.get("by_type", {}).items():
            global_problems_by_type[problem_type] += count
            
        # ç»Ÿè®¡å¥å­é—®é¢˜
        sentence_problems = problem_classification.get("sentence_problems", {})
        for problem_type, count in sentence_problems.get("by_type", {}).items():
            sentence_problems_by_type[problem_type] += count
            
        semantic_sim = news.get("semantic_similarity", 0)
        semantic_similarities.append(semantic_sim)
        
    avg_global_problems = sum(news.get("global_problem_count", 0) for news in successful_evaluations) / total_news
    avg_sentence_problems = sum(news.get("sentence_problem_count", 0) for news in successful_evaluations) / total_news
    avg_total_problems = sum(news.get("total_problem_count", 0) for news in successful_evaluations) / total_news
    avg_semantic_similarity = sum(semantic_similarities) / len(semantic_similarities) if semantic_similarities else 0
    
    return {
        "summary": {
            "total_news_evaluated": total_news,
            "average_global_problems": round(avg_global_problems, 2),
            "average_sentence_problems": round(avg_sentence_problems, 2),
            "average_total_problems": round(avg_total_problems, 2),
            "average_semantic_similarity": round(avg_semantic_similarity, 3),
        },
        "global_problems_classification": {
            "by_type": dict(global_problems_by_type),
            "total_count": sum(global_problems_by_type.values())
        },
        "sentence_problems_classification": {
            "by_type": dict(sentence_problems_by_type),
            "total_count": sum(sentence_problems_by_type.values())
        },
        "similarity_distribution": {
            "min_semantic_similarity": min(semantic_similarities) if semantic_similarities else 0,
            "max_semantic_similarity": max(semantic_similarities) if semantic_similarities else 0,
            "avg_semantic_similarity": round(avg_semantic_similarity, 3),
        },
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŒé‡æ–°é—»è¯„ä¼°å·¥å…·ï¼ˆæ•´ä½“æ–‡æœ¬ + å¥å­çº§åˆ«è¯„ä¼°ï¼‰")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--strict-model", "-sm", default=None, help="ä¸¥æ ¼è¯„ä¼°é˜¶æ®µæ¨¡å‹åç§°")
    parser.add_argument("--cot-model", "-cm", default=None, help="CoTéªŒè¯é˜¶æ®µæ¨¡å‹åç§°")
    parser.add_argument("--similarity-model-path", "-smp", default=None, help="æœ¬åœ°ç›¸ä¼¼åº¦æ¨¡å‹è·¯å¾„")
    parser.add_argument("--gpu-device", "-gpu", default="cuda:0", help="æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ï¼Œå¦‚ cuda:0, cuda:1 ç­‰ï¼Œé»˜è®¤ cuda:0")
    parser.add_argument("--sample", "-s", type=int, default=0, help="å¤„ç†æ ·æœ¬æ•°é‡")
    args = parser.parse_args()
    # python evaluate_2.py --input fused_with_alignment.json --output evaluation_details/åŠ¨ä¹±2gpt-4o-mini.json --strict-model deepseek-chat --cot-model deepseek-chat --similarity-model-path /data1/rjj/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --gpu-device cuda:3
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
        
    print("ğŸ“¥ åŠ è½½æ–°é—»æ•°æ®...")
    news_data = load_news_for_evaluation(args.input)
    if not news_data:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°æ–°é—»æ•°æ®")
        sys.exit(1)
        
    if args.sample > 0:
        news_data = news_data[:args.sample]
        print(f"ğŸ”¬ é‡‡æ ·å¤„ç†å‰ {args.sample} ç¯‡æ–°é—»")
        
    print("ğŸš€ åˆå§‹åŒ–åŒé‡è¯„ä¼°å¤„ç†å™¨...")
    print(f"ğŸ¯ ä½¿ç”¨GPUè®¾å¤‡: {args.gpu_device}")
    processor = DualEvaluationProcessor(
        strict_model_name=args.strict_model, 
        cot_model_name=args.cot_model, 
        local_similarity_model_path=args.similarity_model_path, 
        gpu_device=args.gpu_device
    )
    
    print("ğŸ”„ å¼€å§‹åŒé‡è¯„ä¼°ï¼ˆæ•´ä½“æ–‡æœ¬ + å¥å­çº§åˆ«ï¼‰...")
    evaluated_news = processor.process_news_batch(news_data)
    
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    save_evaluated_news(evaluated_news, args.output)
    
    summary = generate_dual_summary_report(evaluated_news)
    print("\nğŸ“Š åŒé‡è¯„ä¼°æ‘˜è¦:")
    print(f" è¯„ä¼°æ–°é—»æ€»æ•°: {summary['summary']['total_news_evaluated']}")
    print(f" å¹³å‡æ•´ä½“é—®é¢˜æ•°: {summary['summary']['average_global_problems']}")
    print(f" å¹³å‡å¥å­é—®é¢˜æ•°: {summary['summary']['average_sentence_problems']}")
    print(f" å¹³å‡æ€»é—®é¢˜æ•°: {summary['summary']['average_total_problems']}")
    print(f" å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {summary['summary']['average_semantic_similarity']:.3f}")
    
    if summary['global_problems_classification']['total_count'] > 0:
        print(f" æ•´ä½“é—®é¢˜åˆ†ç±»: {summary['global_problems_classification']['by_type']}")
        
    if summary['sentence_problems_classification']['total_count'] > 0:
        print(f" å¥å­é—®é¢˜åˆ†ç±»: {summary['sentence_problems_classification']['by_type']}")
        
    similarity_dist = summary['similarity_distribution']
    print(f" è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€ä½{similarity_dist['min_semantic_similarity']:.3f}, æœ€é«˜{similarity_dist['max_semantic_similarity']:.3f}, å¹³å‡{similarity_dist['avg_semantic_similarity']:.3f}")

if __name__ == "__main__":
    main()